@article{koch2020,
author = {Brandon Koch and David M Vock and Julian Wolfson and Laura Boehm Vock},
title ={Variable selection and estimation in causal inference using {B}ayesian spike and slab priors},
journal = {Statistical Methods in Medical Research},
volume = {29},
number = {9},
pages = {2445-2469},
year = {2020},
doi = {10.1177/0962280219898497},
    abstract = { Unbiased estimation of causal effects with observational data requires adjustment for confounding variables that are related to both the outcome and treatment assignment. Standard variable selection techniques aim to maximize predictive ability of the outcome model, but they ignore covariate associations with treatment and may not adjust for important confounders weakly associated to outcome. We propose a novel method for estimating causal effects that simultaneously considers models for both outcome and treatment, which we call the bilevel spike and slab causal estimator (BSSCE). By using a Bayesian formulation, BSSCE estimates the posterior distribution of all model parameters and provides straightforward and reliable inference. Spike and slab priors are used on each covariate coefficient which aim to minimize the mean squared error of the treatment effect estimator. Theoretical properties of the treatment effect estimator are derived justifying the prior used in BSSCE. Simulations show that BSSCE can substantially reduce mean squared error over numerous methods and performs especially well with large numbers of covariates, including situations where the number of covariates is greater than the sample size. We illustrate BSSCE by estimating the causal effect of vasoactive therapy vs. fluid resuscitation on hypotensive episode length for patients in the Multiparameter Intelligent Monitoring in Intensive Care III critical care database. }
}

@article{koch2018,
author = {Koch, Brandon and Vock, David M. and Wolfson, Julian},
title = {Covariate selection with group lasso and doubly robust estimation of causal effects},
journal = {Biometrics},
volume = {74},
number = {1},
pages = {8-17},
keywords = {Average treatment effect, Causal inference, Group lasso, Variable selection},
doi = {10.1111/biom.12736},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12736},
abstract = {Summary The efficiency of doubly robust estimators of the average causal effect (ACE) of a treatment can be improved by including in the treatment and outcome models only those covariates which are related to both treatment and outcome (i.e., confounders) or related only to the outcome. However, it is often challenging to identify such covariates among the large number that may be measured in a given study. In this article, we propose GLiDeR (Group Lasso and Doubly Robust Estimation), a novel variable selection technique for identifying confounders and predictors of outcome using an adaptive group lasso approach that simultaneously performs coefficient selection, regularization, and estimation across the treatment and outcome models. The selected variables and corresponding coefficient estimates are used in a standard doubly robust ACE estimator. We provide asymptotic results showing that, for a broad class of data generating mechanisms, GLiDeR yields a consistent estimator of the ACE when either the outcome or treatment model is correctly specified. A comprehensive simulation study shows that GLiDeR is more efficient than doubly robust methods using standard variable selection techniques and has substantial computational advantages over a recently proposed doubly robust Bayesian model averaging method. We illustrate our method by estimating the causal treatment effect of bilateral versus single-lung transplant on forced expiratory volume in one year after transplant using an observational registry.},
year = {2018}
}

@article{ishwaran2005,
author = "Ishwaran, Hemant and Rao, J. Sunil",
doi = "10.1214/009053604000001147",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "04",
number = "2",
pages = "730-773",
publisher = "The Institute of Mathematical Statistics",
title = "{Spike and slab variable selection: Frequentist and {B}ayesian strategies}",
volume = "33",
year = "2005"
}

@article{hahn2015,
author = {P. Richard Hahn and Carlos M. Carvalho},
title = {{Decoupling Shrinkage and Selection in {B}ayesian Linear Models: A Posterior Summary Perspective}},
journal = {Journal of the American Statistical Association},
volume = {110},
number = {509},
pages = {435-448},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2014.993077}
}

@Article{Zou2006,
  Title                    = {{The Adaptive Lasso and Its Oracle Properties}},
  Author                   = {Hui Zou},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2006},
  Number                   = {476},
  Pages                    = {1418-1429},
  Volume                   = {101},
  Doi                      = {10.1198/016214506000000735},
  Publisher                = {Taylor \& Francis}
}





@article{winship99,
author = {Winship, Christopher and Morgan, Stephen L.},
title = {THE ESTIMATION OF CAUSAL EFFECTS FROM OBSERVATIONAL DATA},
journal = {Annual Review of Sociology},
volume = {25},
number = {1},
pages = {659-706},
year = {1999},
doi = {10.1146/annurev.soc.25.1.659}
}

@article{rosenbaum83,
	author = {Paul R. Rosenbaum and Donald B. Rubin},
	title = "{The central role of the propensity score in observational studies for causal effects}",
	journal = {Biometrika},
	volume = {70},
	number = {1},
	pages = {41-55},
	year = {1983},
	month = {04},
	abstract = "{The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two- dimensional plot.}",
	issn = {0006-3444},
	doi = {10.1093/biomet/70.1.41},
	url = {https://doi.org/10.1093/biomet/70.1.41},
	
}

@article{Robins1986ANA,
	title={A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	author={James M. Robins},
	journal={Mathematical Modelling},
	year={1986},
	volume={7},
	pages={1393-1512}
}

@article{rosenbaum1985,
	ISSN = {00031305},
	URL = {http://www.jstor.org/stable/2683903},
	abstract = {Matched sampling is a method for selecting units from a large reservoir of potential controls to produce a control group of modest size that is similar to a treated group with respect to the distribution of observed covariates. We illustrate the use of multivariate matching methods in an observational study of the effects of prenatal exposure to barbiturates on subsequent psychological development. A key idea is the use of the propensity score as a distinct matching variable.},
	author = {Paul R. Rosenbaum and Donald B. Rubin},
	journal = {The American Statistician},
	number = {1},
	pages = {33-38},
	publisher = {American Statistical Association, Taylor & Francis, Ltd.},
	title = {Constructing a Control Group Using Multivariate Matched Sampling Methods That Incorporate the Propensity Score},
	urldate = {2022-10-27},
	volume = {39},
	year = {1985}
}

@article{stuart10,
	author = {Elizabeth A. Stuart},
	title = {Matching Methods for Causal Inference: A Review and a Look Forward},
	volume = {25},
	journal = {Statistical Science},
	number = {1},
	publisher = {Institute of Mathematical Statistics},
	pages = {1-21},
	keywords = {observational study, propensity scores, subclassification, weighting},
	year = {2010},
	doi = {10.1214/09-STS313},
	URL = {https://doi.org/10.1214/09-STS313}
}

@article{rubin1978,
	author = {Donald B. Rubin},
	title = {{B}ayesian Inference for Causal Effects: The Role of Randomization},
	volume = {6},
	journal = {The Annals of Statistics},
	number = {1},
	publisher = {Institute of Mathematical Statistics},
	pages = {34-58},
	keywords = {Bayesian, causality, experimentation, inference, missing data, Randomization},
	year = {1978},
	doi = {10.1214/aos/1176344064},
	URL = {https://doi.org/10.1214/aos/1176344064}
}

@article{Crainiceanu2008,
	ISSN = {00063444, 14643510},
	URL = {http://www.jstor.org/stable/20441491},
	abstract = {Often there is substantial uncertainty in the selection of confounders when estimating the association between an exposure and health. We define this type of uncertainty as 'adjustment uncertainty'. We propose a general statistical framework for handling adjustment uncertainty in exposure effect estimation for a large number of confounders, we describe a specific implementation, and we develop associated visualization tools. Theoretical results and simulation studies show that the proposed method provides consistent estimators of the exposure effect and its variance. We also show that, when the goal is to estimate an exposure effect accounting for adjustment uncertainty, Bayesian model averaging with posterior model probabilities approximated using information criteria can fail to estimate the exposure effect and can over- or underestimate its variance. We compare our approach to Bayesian model averaging using time series data on levels of fine particulate matter and mortality.},
	author = {Ciprian M. Crainiceanu and Francesca Dominici and Giovanni Parmigiani},
	journal = {Biometrika},
	number = {3},
	pages = {635-651},
	publisher = {Oxford University Press, Biometrika Trust},
	title = {Adjustment Uncertainty in Effect Estimation},
	urldate = {2022-10-27},
	volume = {95},
	year = {2008}
}

@article{wang2015,
	author = {Wang, Chi and Dominici, Francesca and Parmigiani, Giovanni and Zigler, Corwin Matthew},
	title = {Accounting for uncertainty in confounder and effect modifier selection when estimating average causal effects in generalized linear models},
	journal = {Biometrics},
	volume = {71},
	number = {3},
	pages = {654-665},
	keywords = {Average causal effect, Bayesian adjustment for confounding, Confounder selection, Treatment effect heterogeneity},
	doi = {https://doi.org/10.1111/biom.12315},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12315},
	abstract = {Summary Confounder selection and adjustment are essential elements of assessing the causal effect of an exposure or treatment in observational studies. Building upon work by Wang et al. (2012, Biometrics 68, 661–671) and Lefebvre et al. (2014, Statistics in Medicine 33, 2797–2813), we propose and evaluate a Bayesian method to estimate average causal effects in studies with a large number of potential confounders, relatively few observations, likely interactions between confounders and the exposure of interest, and uncertainty on which confounders and interaction terms should be included. Our method is applicable across all exposures and outcomes that can be handled through generalized linear models. In this general setting, estimation of the average causal effect is different from estimation of the exposure coefficient in the outcome model due to noncollapsibility. We implement a Bayesian bootstrap procedure to integrate over the distribution of potential confounders and to estimate the causal effect. Our method permits estimation of both the overall population causal effect and effects in specified subpopulations, providing clear characterization of heterogeneous exposure effects that may vary considerably across different covariate profiles. Simulation studies demonstrate that the proposed method performs well in small sample size situations with 100–150 observations and 50 covariates. The method is applied to data on 15,060 US Medicare beneficiaries diagnosed with a malignant brain tumor between 2000 and 2009 to evaluate whether surgery reduces hospital readmissions within 30 days of diagnosis.},
	year = {2015}
}

@article{Zigler2014,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/24247140},
	abstract = {Causal inference with observational data frequently relies on the notion of the propensity score (PS) to adjust treatment comparisons for observed confounding factors. As decisions in the era of "big data" are increasingly reliant on large and complex collections of digital data, researchers are frequently confronted with decisions regarding which of a high-dimensional covariate set to include in the PS model to satisfy the assumptions necessary for estimating average causal effects. Typically, simple or ad hoc methods are employed to arrive at a single PS model, without acknowledging the uncertainty associated with the model selection. We propose three Bayesian methods for PS variable selection and model averaging that (a) select relevant variables from a set of candidate variables to include in the PS model and (b) estimate causal treatment effects as weighted averages of estimates under different PS models. The associated weight for each PS model reflects the data-driven support for that model's ability to adjust for the necessary variables. We illustrate features of our proposed approaches with a simulation study, and ultimately use our methods to compare the effectiveness of surgical versus nonsurgical treatment for brain tumors among 2606 Medicare beneficiaries. Supplementary materials for this article are available online.},
	author = {Corwin Matthew Zigler and Francesca Dominici},
	journal = {Journal of the American Statistical Association},
	number = {505},
	pages = {95-107},
	publisher = {American Statistical Association, Taylor & Francis, Ltd.},
	title = {Uncertainty in Propensity Score Estimation: {B}ayesian Methods for Variable Selection and Model-Averaged Causal Effects},
	urldate = {2022-10-27},
	volume = {109},
	year = {2014}
}

@article{Hahn2018,
	author = {P. Richard Hahn and Carlos M. Carvalho and David Puelz and Jingyu He},
	title = {Regularization and Confounding in Linear Regression for Treatment Effect Estimation},
	volume = {13},
	journal = {Bayesian Analysis},
	number = {1},
	publisher = {International Society for Bayesian Analysis},
	pages = {163-182},
	keywords = {Causal inference, observational data, shrinkage estimation},
	year = {2018},
	doi = {10.1214/16-BA1044},
	URL = {https://doi.org/10.1214/16-BA1044}
}

@article{BERGER1990303,
	title = "{Robust {B}ayesian analysis: sensitivity to the prior}",
	journal = "Journal of Statistical Planning and Inference",
	volume = "25",
	number = "3",
	pages = "303 - 328",
	year = "1990",
	issn = "0378-3758",
	doi = "10.1016/0378-3758(90)90079-A",
	author = "James O. Berger"
}

@article{raices_cruz22,
	author = {Raices Cruz, Ivette and Troffaes, Matthias C. M. and Lindström, Johan and Sahlin, Ullrika},
	title = {A robust {B}ayesian bias-adjusted random effects model for consideration of uncertainty about bias terms in evidence synthesis},
	journal = {Statistics in Medicine},
	volume = {41},
	number = {17},
	pages = {3365-3379},
	keywords = {imprecise probability, meta-analysis, risk of bias, robust Bayesian analysis},
	doi = {https://doi.org/10.1002/sim.9422},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9422},
	abstract = {Meta-analysis is a statistical method used in evidence synthesis for combining, analyzing and summarizing studies that have the same target endpoint and aims to derive a pooled quantitative estimate using fixed and random effects models or network models. Differences among included studies depend on variations in target populations (ie, heterogeneity) and variations in study quality due to study design and execution (ie, bias). The risk of bias is usually assessed qualitatively using critical appraisal, and quantitative bias analysis can be used to evaluate the influence of bias on the quantity of interest. We propose a way to consider ignorance or ambiguity in how to quantify bias terms in a bias analysis by characterizing bias with imprecision (as bounds on probability) and use robust Bayesian analysis to estimate the overall effect. Robust Bayesian analysis is here seen as Bayesian updating performed over a set of coherent probability distributions, where the set emerges from a set of bias terms. We show how the set of bias terms can be specified based on judgments on the relative magnitude of biases (ie, low, unclear, and high risk of bias) in one or several domains of the Cochrane's risk of bias table. For illustration, we apply a robust Bayesian bias-adjusted random effects model to an already published meta-analysis on the effect of Rituximab for rheumatoid arthritis from the Cochrane Database of Systematic Reviews.},
	year = {2022}
}

@Article{cruz22_importance,
  author =       {Ivette Raices Cruz and Johan Lindstr{\"o}m and Matthias C. M. Troffaes and Ullrika Sahlin},
  title =        {Iterative importance sampling with {M}arkov Chain {M}onte {C}arlo sampling in robust {B}ayesian analysis},
  journal =      {Computational Statistics \&\ Data Analysis},
  year =         {2022},
  OPTkey =       {},
  volume =    {176},
  OPTnumber =    {},
  pages =     {107558},
  month =     dec,
  OPTnote =      {},
  OPTannote =    {},
  archivePrefix = {arXiv},
  eprint        = {2206.08728},
  primaryClass  = {stat.CO},
  doi = {10.1016/j.csda.2022.107558}
}

@article{xu2015,
	author = {Xiaofan Xu and Malay Ghosh},
	title = {{B}ayesian Variable Selection and Estimation for Group Lasso},
	volume = {10},
	journal = {Bayesian Analysis},
	number = {4},
	publisher = {International Society for Bayesian Analysis},
	pages = {909-936},
	keywords = {Gibbs sampling, group variable selection, median thresholding, spike and slab prior},
	year = {2015},
	doi = {10.1214/14-BA929},
	URL = {https://doi.org/10.1214/14-BA929}
}

@article{HECKMAN1985,
	title = {Alternative methods for evaluating the impact of interventions: An overview},
	journal = {Journal of Econometrics},
	volume = {30},
	number = {1},
	pages = {239-267},
	year = {1985},
	issn = {0304-4076},
	doi = {https://doi.org/10.1016/0304-4076(85)90139-3},
	url = {https://www.sciencedirect.com/science/article/pii/0304407685901393},
	author = {James J. Heckman and Richard Robb},
	abstract = {This paper presents methods for estimating the impact of training on earnings when non-random selection characterizes the enrollment of persons into training. We explore the benefits of cross-section, repeated cross-section and longitudinal data for addressing this problem by considering the assumptions required to use a variety of new and conventional estimators given access to various commonly encountered types of data. We investigate the plausibility of assumptions needed to justify econometric procedures when viewed in the light of prototypical decision rules determining enrollment into training. We examine the robustness of the estimators to choice-based sampling and contamination bias.}
}

@article{albert93,
	ISSN = {01621459},
	URL = {http://www.jstor.org/stable/2290350},
	abstract = {A vast literature in statistics, biometrics, and econometrics is concerned with the analysis of binary and polychotomous response data. The classical approach fits a categorical response regression model using maximum likelihood, and inferences about the model are based on the associated asymptotic theory. The accuracy of classical confidence statements is questionable for small sample sizes. In this article, exact Bayesian methods for modeling categorical response data are developed using the idea of data augmentation. The general approach can be summarized as follows. The probit regression model for binary outcomes is seen to have an underlying normal regression structure on latent continuous data. Values of the latent data can be simulated from suitable truncated normal distributions. If the latent data are known, then the posterior distribution of the parameters can be computed using standard results for normal linear models. Draws from this posterior are used to sample new latent data, and the process is iterated with Gibbs sampling. This data augmentation approach provides a general framework for analyzing binary regression models. It leads to the same simplification achieved earlier for censored regression models. Under the proposed framework, the class of probit regression models can be enlarged by using mixtures of normal distributions to model the latent data. In this normal mixture class, one can investigate the sensitivity of the parameter estimates to the choice of "link function," which relates the linear regression estimate to the fitted probabilities. In addition, this approach allows one to easily fit Bayesian hierarchical models. One specific model considered here reflects the belief that the vector of regression coefficients lies on a smaller dimension linear subspace. The methods can also be generalized to multinomial response models with $J > 2$ categories. In the ordered multinomial model, the J categories are ordered and a model is written linking the cumulative response probabilities with the linear regression structure. In the unordered multinomial model, the latent variables have a multivariate normal distribution with unknown variance-covariance matrix. For both multinomial models, the data augmentation method combined with Gibbs sampling is outlined. This approach is especially attractive for the multivariate probit model, where calculating the likelihood can be difficult.},
	author = {James H. Albert and Siddhartha Chib},
	journal = {Journal of the American Statistical Association},
	number = {422},
	pages = {669-679},
	publisher = {American Statistical Association, Taylor & Francis, Ltd.},
	title = {{B}ayesian Analysis of Binary and Polychotomous Response Data},
	urldate = {2022-10-31},
	volume = {88},
	year = {1993}
}

@article{robinson1988,
	ISSN = {00129682, 14680262},
	URL = {http://www.jstor.org/stable/1912705},
	abstract = {One type of semiparametric regression on an $\scr{R}^{p}\times \scr{R}^{q}\text{-valued}$ random variable (X, Z) is β′X + θ(Z), where β and θ(Z) are an unknown slope coefficient vector and function, and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of β based on incorrect parameterization of θ are generally inconsistent, whereas consistent nonparametric estimators deviate from β by a larger probability order than N-1/2, where N is sample size. An estimator generalizing the ordinary least squares estimator of β is constructed by inserting nonparametric regression estimators in the nonlinear orthogonal projection on Z. Under regularity conditions β̂ is shown to be $N^{1/2}\text{-consistent}$ for β and asymptotically normal, and a consistent estimator of its limiting covariance matrix is given, affording statistical inference that is not only asymptotically valid but has nonzero asymptotic first-order efficiency relative to estimators based on a correctly parameterized θ. We discuss the identification problem and β̂'s efficiency, and report results of a Monte Carlo study of finite-sample performance. While the paper focuses on the simplest interesting setting of multiple regression with independent observations, extensions to other econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models.},
	author = {P. M. Robinson},
	journal = {Econometrica},
	number = {4},
	pages = {931-954},
	publisher = {Wiley, Econometric Society},
	title = {Root-N-Consistent Semiparametric Regression},
	urldate = {2022-11-01},
	volume = {56},
	year = {1988}
}


@article{ZAFFALON20121282,
title = {Evaluating credal classifiers by utility-discounted predictive accuracy},
journal = {International Journal of Approximate Reasoning},
volume = {53},
number = {8},
pages = {1282-1301},
year = {2012},
note = {Imprecise Probability: Theories and Applications (ISIPTA'11)},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2012.06.022},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X12000989},
author = {Marco Zaffalon and Giorgio Corani and Denis Mauá},
abstract = {Predictions made by imprecise-probability models are often indeterminate (that is, set-valued). Measuring the quality of an indeterminate prediction by a single number is important to fairly compare different models, but a principled approach to this problem is currently missing. In this paper we derive, from a set of assumptions, a metric to evaluate the predictions of credal classifiers. These are supervised learning models that issue set-valued predictions. The metric turns out to be made of an objective component, and another that is related to the decision-maker’s degree of risk aversion to the variability of predictions. We discuss when the measure can be rendered independent of such a degree, and provide insights as to how the comparison of classifiers based on the new measure changes with the number of predictions to be made. Finally, we make extensive empirical tests of credal, as well as precise, classifiers by using the new metric. This shows the practical usefulness of the metric, while yielding a first insightful and extensive comparison of credal classifiers.}
}

@article{Basu_2022,
	doi = {10.1007/s13171-022-00287-2}, 
	year = 2022,
	month = {jun},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {85},
  
	number = {1},
  
	pages = {1014-1057},
  
	author = {Tathagata Basu and Matthias C. M. Troffaes and Jochen Einbeck},
  
	title = {A Robust {B}ayesian Analysis of Variable Selection under Prior Ignorance},
  
	journal = {Sankhya A}
}


@InProceedings{schwaferts21a,
  title = 	 {Imprecise Hypothesis-Based {B}ayesian Decision Making with Composite Hypotheses},
  author =       {Schwaferts, Patrick Michael and Augustin, Thomas},
  booktitle = 	 {Proceedings of the Twelveth International Symposium on Imprecise Probability: Theories and Applications},
  pages = 	 {280-288},
  year = 	 {2021},
  editor = 	 {Cano, Andrés and De Bock, Jasper and Miranda, Enrique and Moral, Serafı́n},
  volume = 	 {147},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 jul,
  publisher =    {PMLR},
  url = 	 {https://proceedings.mlr.press/v147/schwaferts21a.html},
  abstract = 	 {Statistical analyses with composite hypotheses are omnipresent in empirical sciences, and a decision-theoretic account is required in order to formally consider their practical relevance. A Bayesian hypothesis-based decision-theoretic analysis requires the specification of a prior distribution, the hypotheses, and a loss function, and determines the optimal decision by minimizing the expected posterior loss of each hypothesis. However, specifying such a decision problem unambiguously is rather difficult as, typically, the relevant information is available only partially. In order to include such incomplete information into the analysis and to facilitate the use of decision-theoretic approaches in applied sciences, this paper extends the framework of hypothesis-based Bayesian decision making with composite hypotheses into the framework of imprecise probabilities, such that imprecise specifications for the prior distribution, for the composite hypotheses, and for the loss function are allowed. Imprecisely specified composite hypotheses are sets of parameter sets that are able to incorporate blurring borders between hypotheses into the analysis. The imprecisely specified prior distribution gets updated via generalized Bayes rule, such that imprecise probabilities of the (imprecise) hypotheses can be calculated. These lead – together with the (imprecise) loss function – to a set-valued expected posterior loss for finding the optimal decision. Beneficially, the result will also indicate whether or not the available information is sufficient to guide the decision unambiguously, without pretending a level of precision that is not available.}
}


@InProceedings{zaffalon20a,
  title = 	 {Structural Causal Models Are (Solvable by) Credal Networks},
  author =       {Zaffalon, Marco and Antonucci, Alessandro and Caba\~nas, Rafael},
  booktitle = 	 {Proceedings of the 10th International Conference on Probabilistic Graphical Models},
  pages = 	 {581-592},
  year = 	 {2020},
  editor = 	 {Jaeger, Manfred and Nielsen, Thomas Dyhre},
  volume = 	 {138},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 sep,
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v138/zaffalon20a/zaffalon20a.pdf},
  url = 	 {https://proceedings.mlr.press/v138/zaffalon20a.html},
  abstract = 	 {A structural causal model is made of endogenous (manifest) and exogenous (latent) variables. We show that endogenous observations induce linear constraints on the probabilities of the exogenous variables. This allows to exactly map a causal model into a credal network. Causal inferences, such as interventions and counterfactuals, can consequently be obtained by standard algorithms for the updating of credal nets. These natively return sharp values in the identifiable case, while intervals corresponding to the exact bounds are produced for unidentifiable queries. A characterization of the causal models that allow the map above to be compactly derived is given, along with a discussion about the scalability for general models. This contribution should be regarded as a systematic approach to represent structural causal models by credal networks and hence to systematically compute causal inferences. A number of demonstrative examples is presented to clarify our methodology. Extensive experiments show that approximate algorithms for credal networks can immediately be used to do causal inference in real-size problems.}
}

@ARTICLE{decoomanzaffalon2004,
  AUTHOR = {{d}e Cooman, Gert and Zaffalon, Marco},
  TITLE = {Updating beliefs with incomplete observations},
  JOURNAL = {Artificial Intelligence},
  YEAR = {2004},
  OPTKEY = {},
  VOLUME = {159},
  NUMBER = {1-2},
  PAGES = {75-125},
  OPTMONTH = {},
  OPTNOTE = {},
  OPTANNOTE = {}
}

@Manual{rjags2023,
    title = {rjags: {B}ayesian Graphical Models using MCMC},
    author = {Martyn Plummer},
    year = {2023},
    note = {{R} package version 4-15},
    url = {https://CRAN.R-project.org/package=rjags}
}
